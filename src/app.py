"""
=================================================================================
MODELO XGBOOST  - PREDICCI√ìN DE PORCENTAJE DE GRASA CORPORAL
=================================================================================
Objetivo: Predecir Fat_Percentage y descubrir los factores m√°s importantes
Autor: Danny Palacios
Link Datos: https://www.kaggle.com/datasets/jockeroika/life-style-data/data
Dataset: 20,000 filas x 54 columnas
=================================================================================
"""


# =============================================================================
# 1. IMPORTACI√ìN DE LIBRER√çAS
# =============================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import xgboost as xgb
from xgboost import plot_importance
import shap
import warnings
warnings.filterwarnings('ignore')

# Configuraci√≥n de visualizaci√≥n
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 1000)

print("=" * 80)
print("MODELO XGBOOST - PREDICCI√ìN DE FAT PERCENTAGE")
print("=" * 80)

# =============================================================================
# 2. CARGA Y EXPLORACI√ìN INICIAL DE DATOS
# =============================================================================

# Cargar el dataset 
df = pd.read_csv('/workspaces/APP-web-ML--Flask-DAN/data/raw/Final_data.csv')

print("\nüìä INFORMACI√ìN GENERAL DEL DATASET")
print("-" * 80)
print(f"Dimensiones: {df.shape[0]} filas x {df.shape[1]} columnas")
print(f"\nPrimeras filas:")
print(df.head())

print(f"\nTipos de datos:")
print(df.dtypes.value_counts())

print(f"\nInformaci√≥n detallada:")
df.info()

# Verificar variable target
print("\nüéØ AN√ÅLISIS DE VARIABLE TARGET: Fat_Percentage")
print("-" * 80)
print(df['Fat_Percentage'].describe())
print(f"\nValores nulos en target: {df['Fat_Percentage'].isnull().sum()}")

# =============================================================================
# 3. AN√ÅLISIS EXPLORATORIO DE DATOS (EDA)
# =============================================================================

print("\nüìà AN√ÅLISIS EXPLORATORIO DE DATOS")
print("-" * 80)

# Valores nulos por columna
print("\nüîç Valores nulos por columna:")
missing = df.isnull().sum()
missing_pct = 100 * df.isnull().sum() / len(df)
missing_table = pd.DataFrame({
    'Valores Nulos': missing,
    'Porcentaje': missing_pct
}).sort_values('Porcentaje', ascending=False)
print(missing_table[missing_table['Valores Nulos'] > 0])

# Estad√≠sticas descriptivas de variables num√©ricas
print("\nüìä Estad√≠sticas descriptivas:")
print(df.describe().T)

# Visualizaci√≥n de la distribuci√≥n del target
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Histograma
axes[0].hist(df['Fat_Percentage'].dropna(), bins=50, edgecolor='black', alpha=0.7)
axes[0].set_xlabel('Fat Percentage', fontsize=12)
axes[0].set_ylabel('Frecuencia', fontsize=12)
axes[0].set_title('Distribuci√≥n de Fat Percentage', fontsize=14, fontweight='bold')
axes[0].axvline(df['Fat_Percentage'].mean(), color='red', linestyle='--', 
                linewidth=2, label=f'Media: {df["Fat_Percentage"].mean():.2f}%')
axes[0].legend()

# Boxplot
axes[1].boxplot(df['Fat_Percentage'].dropna(), vert=True)
axes[1].set_ylabel('Fat Percentage', fontsize=12)
axes[1].set_title('Boxplot de Fat Percentage', fontsize=14, fontweight='bold')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('01_target_distribution.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"\n‚úÖ Gr√°fico guardado: 01_target_distribution.png")

# =============================================================================
# 4. PREPROCESAMIENTO DE DATOS
# =============================================================================

print("\nüîß PREPROCESAMIENTO DE DATOS")
print("-" * 80)

# Crear copia para trabajar
df_clean = df.copy()

# 4.1 Manejar valores nulos en el target
print(f"\nFilas antes de eliminar nulos en target: {len(df_clean)}")
df_clean = df_clean.dropna(subset=['Fat_Percentage'])
print(f"Filas despu√©s de eliminar nulos en target: {len(df_clean)}")

# 4.2 Identificar tipos de columnas
numeric_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()
categorical_cols = df_clean.select_dtypes(include=['object', 'bool']).columns.tolist()

# Remover el target de las features
if 'Fat_Percentage' in numeric_cols:
    numeric_cols.remove('Fat_Percentage')

print(f"\nüìã Variables num√©ricas: {len(numeric_cols)}")
print(numeric_cols[:10], "...")

print(f"\nüìã Variables categ√≥ricas: {len(categorical_cols)}")
print(categorical_cols)

# 4.3 Imputaci√≥n de valores nulos en features num√©ricas
print("\nüî¢ Imputando valores nulos en variables num√©ricas (con la mediana)...")
for col in numeric_cols:
    if df_clean[col].isnull().sum() > 0:
        median_value = df_clean[col].median()
        df_clean[col].fillna(median_value, inplace=True)
        print(f"  - {col}: {df_clean[col].isnull().sum()} nulos ‚Üí completados con {median_value:.2f}")

# 4.4 Imputaci√≥n de valores nulos en features categ√≥ricas
print("\nüìù Imputando valores nulos en variables categ√≥ricas (con la moda)...")
for col in categorical_cols:
    if df_clean[col].isnull().sum() > 0:
        mode_value = df_clean[col].mode()[0]
        df_clean[col].fillna(mode_value, inplace=True)
        print(f"  - {col}: completado con '{mode_value}'")

# 4.5 Encoding de variables categ√≥ricas
print("\nüî§ Aplicando Label Encoding a variables categ√≥ricas...")
label_encoders = {}

for col in categorical_cols:
    le = LabelEncoder()
    df_clean[col] = le.fit_transform(df_clean[col].astype(str))
    label_encoders[col] = le
    print(f"  - {col}: {len(le.classes_)} categor√≠as √∫nicas")

# 4.6 Feature Engineering
print("\n‚öôÔ∏è FEATURE ENGINEERING")
print("-" * 80)

# Calcular BMI si no existe (validaci√≥n)
if 'BMI' not in df_clean.columns and 'Weight (kg)' in df_clean.columns and 'Height (m)' in df_clean.columns:
    df_clean['BMI'] = df_clean['Weight (kg)'] / (df_clean['Height (m)'] ** 2)
    print("‚úÖ BMI calculado")

# Crear nuevas features relevantes
if 'Calories_Burned' in df_clean.columns and 'Session_Duration (hours)' in df_clean.columns:
    df_clean['Calories_Per_Hour'] = df_clean['Calories_Burned'] / (df_clean['Session_Duration (hours)'] + 0.001)
    print("‚úÖ Calories_Per_Hour creado")

if 'Avg_BPM' in df_clean.columns and 'Resting_BPM' in df_clean.columns:
    df_clean['BPM_Elevation'] = df_clean['Avg_BPM'] - df_clean['Resting_BPM']
    print("‚úÖ BPM_Elevation creado")

if 'Proteins' in df_clean.columns and 'Weight (kg)' in df_clean.columns:
    df_clean['Protein_Per_Kg'] = df_clean['Proteins'] / (df_clean['Weight (kg)'] + 0.001)
    print("‚úÖ Protein_Per_Kg creado")

if 'Calories' in df_clean.columns and 'Weight (kg)' in df_clean.columns:
    df_clean['Calorie_Density'] = df_clean['Calories'] / (df_clean['Weight (kg)'] + 0.001)
    print("‚úÖ Calorie_Density creado")

# Ratio de macronutrientes
if all(col in df_clean.columns for col in ['Carbs', 'Proteins', 'Fats']):
    total_macros = df_clean['Carbs'] + df_clean['Proteins'] + df_clean['Fats'] + 0.001
    df_clean['Carbs_Ratio'] = df_clean['Carbs'] / total_macros
    df_clean['Protein_Ratio'] = df_clean['Proteins'] / total_macros
    df_clean['Fat_Ratio'] = df_clean['Fats'] / total_macros
    print("‚úÖ Ratios de macronutrientes creados")

print(f"\nüìä Dimensiones finales del dataset: {df_clean.shape}")

# =============================================================================
# 5. PREPARACI√ìN DE DATOS PARA MODELADO
# =============================================================================

print("\nüé≤ PREPARACI√ìN DE DATOS PARA MODELADO")
print("-" * 80)

# Separar features y target
X = df_clean.drop('Fat_Percentage', axis=1)
y = df_clean['Fat_Percentage']

print(f"Features (X): {X.shape}")
print(f"Target (y): {y.shape}")

# Divisi√≥n train/test (80/20)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, shuffle=True
)

print(f"\n‚úÇÔ∏è Divisi√≥n de datos:")
print(f"  - Entrenamiento: {X_train.shape[0]} muestras ({X_train.shape[0]/len(X)*100:.1f}%)")
print(f"  - Prueba: {X_test.shape[0]} muestras ({X_test.shape[0]/len(X)*100:.1f}%)")

# =============================================================================
# 6. AN√ÅLISIS DE CORRELACIONES
# =============================================================================

print("\nüîó AN√ÅLISIS DE CORRELACIONES CON FAT_PERCENTAGE")
print("-" * 80)

# Calcular correlaciones con el target
correlations = df_clean.corr()['Fat_Percentage'].sort_values(ascending=False)
print("\nüîù Top 15 variables m√°s correlacionadas:")
print(correlations.head(15))

print("\nüîª Top 10 variables menos correlacionadas:")
print(correlations.tail(10))

# Visualizar top correlaciones
fig, ax = plt.subplots(figsize=(10, 8))
top_corr = correlations.head(20)
colors = ['green' if x > 0 else 'red' for x in top_corr.values]
top_corr.plot(kind='barh', color=colors, ax=ax)
ax.set_xlabel('Correlaci√≥n con Fat_Percentage', fontsize=12)
ax.set_title('Top 20 Variables m√°s Correlacionadas con Fat Percentage', 
             fontsize=14, fontweight='bold')
ax.axvline(x=0, color='black', linestyle='-', linewidth=0.8)
plt.tight_layout()
plt.savefig('02_correlations.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"\n‚úÖ Gr√°fico guardado: 02_correlations.png")

# =============================================================================
# 7. ENTRENAMIENTO DEL MODELO BASE XGBOOST
# =============================================================================

print("\nüöÄ ENTRENAMIENTO DEL MODELO BASE XGBOOST")
print("-" * 80)

# Configuraci√≥n inicial del modelo
xgb_model = xgb.XGBRegressor(
    objective='reg:squarederror',  # Funci√≥n objetivo para regresi√≥n
    n_estimators=100,              # N√∫mero de √°rboles
    learning_rate=0.1,             # Tasa de aprendizaje
    max_depth=6,                   # Profundidad m√°xima de √°rboles
    min_child_weight=1,            # Peso m√≠nimo en nodos hijos
    subsample=0.8,                 # Fracci√≥n de muestras por √°rbol
    colsample_bytree=0.8,          # Fracci√≥n de features por √°rbol
    random_state=42,
    n_jobs=-1,                     # Usar todos los cores disponibles
    verbosity=1
)

print("‚è≥ Entrenando modelo base...")
xgb_model.fit(
    X_train, y_train,
    eval_set=[(X_train, y_train), (X_test, y_test)],
    verbose=False
)

# Predicciones
y_pred_train = xgb_model.predict(X_train)
y_pred_test = xgb_model.predict(X_test)

# Evaluaci√≥n del modelo base
print("\nüìä EVALUACI√ìN DEL MODELO BASE")
print("-" * 80)

# M√©tricas de entrenamiento
rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))
mae_train = mean_absolute_error(y_train, y_pred_train)
r2_train = r2_score(y_train, y_pred_train)
mape_train = np.mean(np.abs((y_train - y_pred_train) / y_train)) * 100

print("üèãÔ∏è CONJUNTO DE ENTRENAMIENTO:")
print(f"  - RMSE: {rmse_train:.4f}")
print(f"  - MAE:  {mae_train:.4f}")
print(f"  - R¬≤:   {r2_train:.4f}")
print(f"  - MAPE: {mape_train:.2f}%")

# M√©tricas de prueba
rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))
mae_test = mean_absolute_error(y_test, y_pred_test)
r2_test = r2_score(y_test, y_pred_test)
mape_test = np.mean(np.abs((y_test - y_pred_test) / y_test)) * 100

print("\nüß™ CONJUNTO DE PRUEBA:")
print(f"  - RMSE: {rmse_test:.4f}")
print(f"  - MAE:  {mae_test:.4f}")
print(f"  - R¬≤:   {r2_test:.4f}")
print(f"  - MAPE: {mape_test:.2f}%")

# Verificar overfitting
overfit_diff = abs(r2_train - r2_test)
print(f"\n‚ö†Ô∏è Diferencia R¬≤ (Train - Test): {overfit_diff:.4f}")
if overfit_diff > 0.1:
    print("   ‚Üí Posible overfitting. Considera ajustar hiperpar√°metros.")
else:
    print("   ‚Üí ‚úÖ Modelo generaliza bien.")

# =============================================================================
# 8. VALIDACI√ìN CRUZADA
# =============================================================================

print("\nüîÑ VALIDACI√ìN CRUZADA (K-Fold = 5)")
print("-" * 80)

kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# Evaluar con diferentes m√©tricas
cv_rmse = -cross_val_score(xgb_model, X_train, y_train, 
                           cv=kfold, scoring='neg_root_mean_squared_error')
cv_mae = -cross_val_score(xgb_model, X_train, y_train, 
                          cv=kfold, scoring='neg_mean_absolute_error')
cv_r2 = cross_val_score(xgb_model, X_train, y_train, 
                        cv=kfold, scoring='r2')

print(f"üìä Resultados de Cross-Validation:")
print(f"  - RMSE: {cv_rmse.mean():.4f} (+/- {cv_rmse.std():.4f})")
print(f"  - MAE:  {cv_mae.mean():.4f} (+/- {cv_mae.std():.4f})")
print(f"  - R¬≤:   {cv_r2.mean():.4f} (+/- {cv_r2.std():.4f})")

# =============================================================================
# 9. OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS
# =============================================================================

print("\nüéõÔ∏è OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS (Grid Search)")
print("-" * 80)
print("‚è≥ Este proceso puede tomar varios minutos...")

from sklearn.model_selection import GridSearchCV

# Definir grid de par√°metros
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [4, 6, 8],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.7, 0.8, 0.9],
    'colsample_bytree': [0.7, 0.8, 0.9],
    'min_child_weight': [1, 3, 5]
}

# Grid Search con validaci√≥n cruzada
grid_search = GridSearchCV(
    estimator=xgb.XGBRegressor(
        objective='reg:squarederror',
        random_state=42,
        n_jobs=-1
    ),
    param_grid=param_grid,
    cv=3,
    scoring='neg_root_mean_squared_error',
    verbose=1,
    n_jobs=-1
)

grid_search.fit(X_train, y_train)

print("\nüèÜ MEJORES HIPERPAR√ÅMETROS ENCONTRADOS:")
print("-" * 80)
for param, value in grid_search.best_params_.items():
    print(f"  - {param}: {value}")

print(f"\nüìà Mejor RMSE (CV): {-grid_search.best_score_:.4f}")

# Modelo optimizado
best_model = grid_search.best_estimator_

# Predicciones con modelo optimizado
y_pred_train_opt = best_model.predict(X_train)
y_pred_test_opt = best_model.predict(X_test)

# Evaluaci√≥n del modelo optimizado
print("\nüìä EVALUACI√ìN DEL MODELO OPTIMIZADO")
print("-" * 80)

rmse_test_opt = np.sqrt(mean_squared_error(y_test, y_pred_test_opt))
mae_test_opt = mean_absolute_error(y_test, y_pred_test_opt)
r2_test_opt = r2_score(y_test, y_pred_test_opt)
mape_test_opt = np.mean(np.abs((y_test - y_pred_test_opt) / y_test)) * 100

print("üß™ CONJUNTO DE PRUEBA (Modelo Optimizado):")
print(f"  - RMSE: {rmse_test_opt:.4f} (Base: {rmse_test:.4f})")
print(f"  - MAE:  {mae_test_opt:.4f} (Base: {mae_test:.4f})")
print(f"  - R¬≤:   {r2_test_opt:.4f} (Base: {r2_test:.4f})")
print(f"  - MAPE: {mape_test_opt:.2f}% (Base: {mape_test:.2f}%)")

improvement = ((rmse_test - rmse_test_opt) / rmse_test) * 100
print(f"\n‚ú® Mejora en RMSE: {improvement:.2f}%")

# =============================================================================
# 10. IMPORTANCIA DE FEATURES
# =============================================================================

print("\n‚≠ê IMPORTANCIA DE FEATURES")
print("-" * 80)

# Obtener importancia de features
feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': best_model.feature_importances_
}).sort_values('Importance', ascending=False)

print("\nüîù Top 20 Features M√°s Importantes:")
print(feature_importance.head(20).to_string(index=False))

# Visualizaci√≥n de importancia
fig, axes = plt.subplots(1, 2, figsize=(16, 8))

# Gr√°fico 1: Top 20 features
top_20 = feature_importance.head(20)
axes[0].barh(range(len(top_20)), top_20['Importance'], color='skyblue')
axes[0].set_yticks(range(len(top_20)))
axes[0].set_yticklabels(top_20['Feature'])
axes[0].invert_yaxis()
axes[0].set_xlabel('Importancia', fontsize=12)
axes[0].set_title('Top 20 Features M√°s Importantes', fontsize=14, fontweight='bold')
axes[0].grid(axis='x', alpha=0.3)

# Gr√°fico 2: XGBoost built-in importance plot
plot_importance(best_model, max_num_features=20, ax=axes[1], 
                importance_type='gain', show_values=False)
axes[1].set_title('Feature Importance (Gain)', fontsize=14, fontweight='bold')

plt.tight_layout()
plt.savefig('03_feature_importance.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"\n‚úÖ Gr√°fico guardado: 03_feature_importance.png")

# =============================================================================
# 11. AN√ÅLISIS SHAP (Interpretabilidad)
# =============================================================================

print("\nüîç AN√ÅLISIS SHAP - INTERPRETABILIDAD DEL MODELO")
print("-" * 80)
print("‚è≥ Calculando valores SHAP (puede tomar un momento)...")

# Crear explainer SHAP
explainer = shap.TreeExplainer(best_model)

# Calcular SHAP values (usar muestra para eficiencia)
sample_size = min(1000, len(X_test))
X_test_sample = X_test.sample(n=sample_size, random_state=42)
shap_values = explainer.shap_values(X_test_sample)

# Summary plot
fig, ax = plt.subplots(figsize=(12, 8))
shap.summary_plot(shap_values, X_test_sample, plot_type="bar", show=False)
plt.title('SHAP Feature Importance', fontsize=14, fontweight='bold', pad=20)
plt.tight_layout()
plt.savefig('04_shap_importance.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"‚úÖ Gr√°fico guardado: 04_shap_importance.png")

# SHAP summary plot detallado
fig, ax = plt.subplots(figsize=(12, 8))
shap.summary_plot(shap_values, X_test_sample, show=False)
plt.title('SHAP Summary Plot - Impacto y Distribuci√≥n', fontsize=14, fontweight='bold', pad=20)
plt.tight_layout()
plt.savefig('05_shap_summary.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"‚úÖ Gr√°fico guardado: 05_shap_summary.png")

# =============================================================================
# 12. VISUALIZACI√ìN DE PREDICCIONES
# =============================================================================

print("\nüìâ VISUALIZACI√ìN DE PREDICCIONES")
print("-" * 80)

fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# Gr√°fico 1: Predicciones vs Valores Reales
axes[0, 0].scatter(y_test, y_pred_test_opt, alpha=0.5, s=20)
axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 
                'r--', lw=2, label='Predicci√≥n Perfecta')
axes[0, 0].set_xlabel('Fat Percentage Real', fontsize=11)
axes[0, 0].set_ylabel('Fat Percentage Predicho', fontsize=11)
axes[0, 0].set_title(f'Predicciones vs Valores Reales\n(R¬≤ = {r2_test_opt:.4f})', 
                     fontsize=12, fontweight='bold')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

# Gr√°fico 2: Distribuci√≥n de residuos
residuals = y_test - y_pred_test_opt
axes[0, 1].hist(residuals, bins=50, edgecolor='black', alpha=0.7)
axes[0, 1].axvline(x=0, color='red', linestyle='--', linewidth=2, label='Residuo = 0')
axes[0, 1].set_xlabel('Residuos (Real - Predicho)', fontsize=11)
axes[0, 1].set_ylabel('Frecuencia', fontsize=11)
axes[0, 1].set_title(f'Distribuci√≥n de Residuos\n(Media = {residuals.mean():.4f})', 
                     fontsize=12, fontweight='bold')
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)

# Gr√°fico 3: Residuos vs Predicciones
axes[1, 0].scatter(y_pred_test_opt, residuals, alpha=0.5, s=20)
axes[1, 0].axhline(y=0, color='red', linestyle='--', linewidth=2)
axes[1, 0].set_xlabel('Fat Percentage Predicho', fontsize=11)
axes[1, 0].set_ylabel('Residuos', fontsize=11)
axes[1, 0].set_title('Residuos vs Predicciones\n(Homocedasticidad)', 
                     fontsize=12, fontweight='bold')
axes[1, 0].grid(True, alpha=0.3)

# Gr√°fico 4: Errores absolutos
errors = np.abs(residuals)
axes[1, 1].hist(errors, bins=50, edgecolor='black', alpha=0.7, color='orange')
axes[1, 1].axvline(x=mae_test_opt, color='red', linestyle='--', 
                   linewidth=2, label=f'MAE = {mae_test_opt:.4f}')
axes[1, 1].set_xlabel('Error Absoluto', fontsize=11)
axes[1, 1].set_ylabel('Frecuencia', fontsize=11)
axes[1, 1].set_title('Distribuci√≥n de Errores Absolutos', 
                     fontsize=12, fontweight='bold')
axes[1, 1].legend()
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('06_predictions_analysis.png', dpi=300, bbox_inches='tight')
plt.show()

print(f"‚úÖ Gr√°fico guardado: 06_predictions_analysis.png")

# =============================================================================
# 13. GUARDAR EL MODELO
# =============================================================================

print("\nüíæ GUARDANDO MODELO Y RESULTADOS")
print("-" * 80)

# Guardar modelo
import joblib
joblib.dump(best_model, 'xgboost_fat_percentage_model.pkl')
print("‚úÖ Modelo guardado: xgboost_fat_percentage_model.pkl")

# Guardar encoders
joblib.dump(label_encoders, 'label_encoders.pkl')
print("‚úÖ Encoders guardados: label_encoders.pkl")

# Guardar feature importance
feature_importance.to_csv('feature_importance.csv', index=False)
print("‚úÖ Feature importance guardado: feature_importance.csv")

# Guardar m√©tricas
metrics_summary = pd.DataFrame({
    'Metric': ['RMSE', 'MAE', 'R¬≤', 'MAPE'],
    'Train': [
        np.sqrt(mean_squared_error(y_train, y_pred_train_opt)),
        mean_absolute_error(y_train, y_pred_train_opt),
        r2_score(y_train, y_pred_train_opt),
        np.mean(np.abs((y_train - y_pred_train_opt) / y_train)) * 100
    ],
    'Test': [rmse_test_opt, mae_test_opt, r2_test_opt, mape_test_opt]
})
metrics_summary.to_csv('model_metrics.csv', index=False)
print("‚úÖ M√©tricas guardadas: model_metrics.csv")

# =============================================================================
# 14. RESUMEN FINAL
# =============================================================================

print("\n" + "=" * 80)
print("üìã RESUMEN FINAL DEL MODELO")
print("=" * 80)

print(f"""
üéØ OBJETIVO: Predecir Fat_Percentage corporal

üìä DATOS:
  - Total de muestras: {len(df_clean):,}
  - Features utilizados: {X.shape[1]}
  - Train/Test split: 80%/20%

üèÜ MEJOR MODELO (XGBoost Optimizado):
  - n_estimators: {best_model.n_estimators}
  - max_depth: {best_model.max_depth}
  - learning_rate: {best_model.learning_rate}

üìà M√âTRICAS DE EVALUACI√ìN (Test Set):
  - RMSE: {rmse_test_opt:.4f}
  - MAE:  {mae_test_opt:.4f}
  - R¬≤:   {r2_test_opt:.4f}
  - MAPE: {mape_test_opt:.2f}%

‚≠ê TOP 5 FACTORES M√ÅS IMPORTANTES:
""")

for i, row in feature_importance.head(5).iterrows():
    print(f"  {i+1}. {row['Feature']}: {row['Importance']:.4f}")

print(f"""
üí° INTERPRETACI√ìN:
  - El modelo explica el {r2_test_opt*100:.2f}% de la varianza en Fat_Percentage
  - Error promedio de predicci√≥n: {mae_test_opt:.2f} puntos porcentuales
  - {'‚úÖ Buen ajuste' if r2_test_opt > 0.7 else '‚ö†Ô∏è Considerar m√°s features o datos'}

üìÅ ARCHIVOS GENERADOS:
  - xgboost_fat_percentage_model.pkl (modelo entrenado)
  - label_encoders.pkl (encoders para variables categ√≥ricas)
  - feature_importance.csv (importancia de variables)
  - model_metrics.csv (m√©tricas de evaluaci√≥n)
  - 6 gr√°ficos PNG (an√°lisis visual completo)

üöÄ PARA USAR EL MODELO:
""")

print("""
# Cargar el modelo
import joblib
model = joblib.load('xgboost_fat_percentage_model.pkl')
encoders = joblib.load('label_encoders.pkl')

# Hacer predicciones
new_data = pd.DataFrame({...})  # Tus nuevos datos
prediction = model.predict(new_data)
print(f'Fat Percentage predicho: {prediction[0]:.2f}%')
""")

print("=" * 80)
print("‚úÖ AN√ÅLISIS COMPLETADO EXITOSAMENTE")
print("=" * 80)

# =============================================================================
# 15. AN√ÅLISIS ADICIONAL: INSIGHTS SOBRE FAT PERCENTAGE
# =============================================================================

print("\n\nüî¨ AN√ÅLISIS ADICIONAL: INSIGHTS CLAVE SOBRE FAT PERCENTAGE")
print("=" * 80)

# An√°lisis por rangos de Fat Percentage
df_clean['Fat_Category'] = pd.cut(
    df_clean['Fat_Percentage'],
    bins=[0, 15, 25, 35, 100],
    labels=['Bajo (<15%)', 'Normal (15-25%)', 'Alto (25-35%)', 'Muy Alto (>35%)']
)

print("\nüìä Distribuci√≥n por categor√≠a:")
print(df_clean['Fat_Category'].value_counts().sort_index())

# An√°lisis de las top features por categor√≠a
if 'BMI' in df_clean.columns:
    print("\nüèãÔ∏è Promedio de BMI por categor√≠a de grasa:")
    print(df_clean.groupby('Fat_Category')['BMI'].mean().sort_index())

if 'Workout_Frequency (days/week)' in df_clean.columns:
    print("\nüèÉ Promedio de frecuencia de ejercicio por categor√≠a:")
    print(df_clean.groupby('Fat_Category')['Workout_Frequency (days/week)'].mean().sort_index())

if 'Protein_Ratio' in df_clean.columns:
    print("\nü•© Promedio de ratio de prote√≠nas por categor√≠a:")
    print(df_clean.groupby('Fat_Category')['Protein_Ratio'].mean().sort_index())

# =============================================================================
# 16. RECOMENDACIONES Y PR√ìXIMOS PASOS
# =============================================================================

print("\n\nüí° RECOMENDACIONES Y PR√ìXIMOS PASOS")
print("=" * 80)

recommendations = """
üìå FACTORES CLAVE IDENTIFICADOS:
   Los factores m√°s importantes para predecir Fat_Percentage incluyen
   variables relacionadas con:
   - Composici√≥n corporal (BMI, Weight, Height)
   - Nutrici√≥n (ratios de macronutrientes, ingesta cal√≥rica)
   - Actividad f√≠sica (frecuencia, intensidad, tipo de ejercicio)
   - Metabolismo (BPM en reposo, edad)

üéØ PARA MEJORAR A√öN M√ÅS EL MODELO:
   1. Recolectar m√°s datos si R¬≤ < 0.80
   2. Crear interacciones entre features importantes (ej: BMI * Workout_Frequency)
   3. Probar ensemble con otros algoritmos (Random Forest, LightGBM)
   4. Aplicar feature selection m√°s riguroso (eliminar features poco importantes)
   5. Considerar t√©cnicas de regularizaci√≥n adicionales

üìä APLICACIONES PR√ÅCTICAS:
   - Sistemas de recomendaci√≥n personalizados de dieta y ejercicio
   - Evaluaci√≥n de progreso en programas fitness
   - Identificaci√≥n de factores de riesgo para obesidad
   - Optimizaci√≥n de planes nutricionales

‚ö†Ô∏è CONSIDERACIONES IMPORTANTES:
   - Este modelo es para prop√≥sitos educativos/informativos
   - No reemplaza evaluaci√≥n m√©dica profesional
   - Los resultados dependen de la calidad de los datos de entrada
   - Validar en datos nuevos antes de uso en producci√≥n
"""

print(recommendations)

# =============================================================================
# 17. FUNCI√ìN DE PREDICCI√ìN EJEMPLO
# =============================================================================

print("\n\nüîß FUNCI√ìN DE PREDICCI√ìN PERSONALIZADA")
print("=" * 80)

example_code = """
def predict_fat_percentage(model, encoders, user_data):
    \"\"\"
    Predice el porcentaje de grasa corporal para un nuevo usuario
    
    Parameters:
    -----------
    model : XGBoost model
        Modelo entrenado
    encoders : dict
        Diccionario de LabelEncoders
    user_data : dict
        Datos del usuario con todas las features
    
    Returns:
    --------
    float : Porcentaje de grasa predicho
    dict : Informaci√≥n adicional (categor√≠a, recomendaciones)
    \"\"\"
    import pandas as pd
    import numpy as np
    
    # Convertir a DataFrame
    df = pd.DataFrame([user_data])
    
    # Aplicar encoders a variables categ√≥ricas
    for col, encoder in encoders.items():
        if col in df.columns:
            df[col] = encoder.transform(df[col].astype(str))
    
    # Crear features engineered (igual que en entrenamiento)
    if 'BMI' not in df.columns:
        df['BMI'] = df['Weight (kg)'] / (df['Height (m)'] ** 2)
    
    if 'Calories_Per_Hour' not in df.columns:
        df['Calories_Per_Hour'] = df['Calories_Burned'] / (df['Session_Duration (hours)'] + 0.001)
    
    # ... (agregar todas las features creadas en el entrenamiento)
    
    # Predecir
    prediction = model.predict(df)[0]
    
    # Categorizar
    if prediction < 15:
        category = "Bajo"
        recommendation = "Mant√©n tu rutina actual"
    elif prediction < 25:
        category = "Normal"
        recommendation = "Excelente rango saludable"
    elif prediction < 35:
        category = "Alto"
        recommendation = "Considera aumentar actividad f√≠sica"
    else:
        category = "Muy Alto"
        recommendation = "Consulta con un profesional de salud"
    
    return {
        'fat_percentage': round(prediction, 2),
        'category': category,
        'recommendation': recommendation
    }

# EJEMPLO DE USO:
# ================

# Cargar modelo y encoders
import joblib
model = joblib.load('xgboost_fat_percentage_model.pkl')
encoders = joblib.load('label_encoders.pkl')

# Datos de ejemplo de un nuevo usuario
new_user = {
    'Age': 30,
    'Gender': 'Male',
    'Weight (kg)': 80,
    'Height (m)': 1.75,
    'Workout_Frequency (days/week)': 4,
    'Calories': 2500,
    'Proteins': 150,
    'Carbs': 250,
    'Fats': 80,
    # ... (todas las dem√°s features requeridas)
}

# Hacer predicci√≥n
result = predict_fat_percentage(model, encoders, new_user)
print(f"Porcentaje de grasa predicho: {result['fat_percentage']}%")
print(f"Categor√≠a: {result['category']}")
print(f"Recomendaci√≥n: {result['recommendation']}")
"""

print(example_code)

print("\n" + "=" * 80)
print("üéâ SCRIPT COMPLETADO - ¬°Listo para usar!")
print("=" * 80)

print("""
üìö DOCUMENTACI√ìN ADICIONAL:
   - XGBoost: https://xgboost.readthedocs.io/
   - SHAP: https://shap.readthedocs.io/
   - Scikit-learn: https://scikit-learn.org/

üí¨ ¬øPreguntas o ajustes? Puedes:
   - Modificar hiperpar√°metros en la secci√≥n 9
   - Ajustar feature engineering en la secci√≥n 4.6
   - Cambiar la estrategia de validaci√≥n en la secci√≥n 8
   - Personalizar visualizaciones en las secciones 10-12
""")
